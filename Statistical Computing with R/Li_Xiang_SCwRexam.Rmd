---
title: "Exam Solution"
author: "Xiang Li(4013115)"
date: "2024-01-04"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(tidy = TRUE, tidy.opts = list(width.cutoff = 55), error = TRUE)
```
# Execise 1
## 1
```{r}
polls_poland_df = read.csv('polls_poland.csv')
```
## 2
```{r}
polls_poland_df$Fieldwork.End = as.Date(polls_poland_df$Fieldwork.End, format = '%Y-%m-%d')
polls_poland_df = polls_poland_df[(polls_poland_df$Fieldwork.End >= '2023-10-01') & (polls_poland_df$Fieldwork.End <= '2023-10-31'), c("Polling.Firm", "Fieldwork.End", "Sample.Size", "ZP", "KO", "TD", "Lewica", "Konfederacja")]
polls_poland_df
nrow(polls_poland_df)
```
21 opinion polls are left now.

## 3
```{r}
mean_df = as.data.frame(apply(polls_poland_df[, 4:8], 2, mean, na.rm = T))
colnames(mean_df) = c('Ave_Share')
mean_df
```

## 4
```{r, fig.width=9, fig.height=8, fig.align='center'}
mean_df['Other parties', 'Ave_Share'] = 100 - sum(mean_df$Ave_Share)
mean_df$Name = c('United Right', 'Civic Platform', 'Third Way', 'The Left', 'Confederation', 'Other parties')
mean_df = mean_df[order(mean_df$Ave_Share, decreasing = T), ]
barplot(height = mean_df$Ave_Share, names.arg = mean_df$Name, col = 1:6, main = 'Distribution of Average Share on Opinion Polls, Oct.2023', xlab = 'party', ylab = 'average share')
```

## 5
```{r}
act_share = c(35.4, 30.7, 14.4, 8.6, 7.2)
mean_df$Act_Share = c(act_share, 100-sum(act_share))
mean_df$compare = (mean_df$Act_Share > mean_df$Ave_Share) * 1
mean_df
```
Based on the table above, Civic Platform and Third Way obtained more votes than predicted by the opinion polls, United Right, The Left and Confederation obtained less votes than expected.

# Execise 2
## 1
The answer is on the attached exam paper.

## 2
```{r}
x_seq = c(10.4, -8.74, 3.58, -1.98)
y = sum(x_seq)
k = length(x_seq)
while (y <= 172000) {
  k = k+1
  x_seq[k] = (3*x_seq[k-1] - 5*x_seq[k-3])/4
  y = sum(x_seq)
}
length(x_seq)
```
I need to cumulate 67 elements from $x_k$ to make $y_k$ exceed 172000.

## 3
```{r}
k
y
```
$k = 67$ and $y_k = 400344.6$.

# Execise 3
## 1
```{r}
f_X = function(x, p, alpha){
  (-1/log(p))*(alpha*(1-p)*exp(-alpha*x))/(1-(1-p)*exp(-alpha*x))
}
f_X(x = 2, p = 0.7, alpha = 1)
```
P(X = 2) is greater than 0.5 is False, because X is a continuous random variable and the probability of that a continuous random variable gets a single value is 0, so $P(X = 2) = 0$.

## 2
```{r}
log_lik = function(x_v, p, alpha){
  if (any(x_v < 0)) {
    stop("There is negative value in x!!!")
  }
  if ((p <= 0) | (p >= 1) | (alpha <= 0)){
    stop("Argument p or alpha is invalid!!!")
  }
  n = length(x_v)
  result = n * log(-alpha*(1-p)/log(p)) - sum(log(exp(alpha*x_v)+p-1))
  return(result)
}
```
## 3
```{r}
sample_v = c(0.1, 0.2, 0.4, 1.3, 0.1, 1.2, 1.6, 0.5, 0.4, 0.1, 0.1, 0.1)
```
```{r}
neg_log_lik = function(para, x_v){
  p = para[1]
  alpha = para[2]
  return(-1*log_lik(x_v, p, alpha))
}
mle = optim(c(0.1, 1), neg_log_lik, method = 'L-BFGS-B', lower = c(1e-5, 1e-5), upper = c(1-1e-5, Inf), x_v = sample_v)
c(p_hat = pnorm(mle$par[1]), alpha_hat = exp(mle$par[2]), loglikelihood = -1*mle$value)
```
$\hat{p} = 0.6647309$, $\hat{\alpha} = 4.9145432$ and $loglikelihood = -3.7534383$.

## 4
The answer is on the attached exam paper.

## 5
```{r}
neg_log_lik1 = function(para, x_v){
  p = pnorm(para[1])
  alpha = exp(para[2])
  return(-1*log_lik(x_v, p, alpha))
}
mle1 = optim(c(0.1, 1), neg_log_lik1, x_v = sample_v)
c(p_hat = pnorm(mle1$par[1]), alpha_hat = exp(mle1$par[2]), loglikelihood = -1*mle1$value)
```
$\hat{p} = 0.425438$, $\hat{\alpha} = 1.592248$ and $loglikelihood = -3.753438$.

## 6
```{r}
list(mle_const = c(p_hat = pnorm(mle$par[1]), alpha_hat = exp(mle$par[2]), loglikelihood = -1*mle$value), mle_repara = c(p_hat = pnorm(mle1$par[1]), alpha_hat = exp(mle1$par[2]), loglikelihood = -1*mle1$value))
```
I can't say which one is better.

I could say the optimization which has larger loglikelihood value is better than the other one. But based on the results above, the final loglikelihood values from the two methods are almost same, so I can't say which one is better.




